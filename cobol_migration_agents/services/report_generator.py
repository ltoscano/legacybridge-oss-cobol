"""Report generation service for COBOL Migration Agents."""

import aiofiles
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional

from ..config.settings import Settings
from ..models.cobol_models import CobolFile, CobolAnalysis
from ..models.java_models import JavaFile
from ..models.dependency_models import DependencyMap


class ReportGenerator:
    """
    Service for generating comprehensive migration reports.
    
    Creates detailed markdown reports with migration statistics,
    file mappings, dependency analysis, and recommendations.
    """
    
    def __init__(self, settings: Settings):
        """Initialize the report generator."""
        self.settings = settings
    
    async def generate_migration_report(
        self,
        cobol_files: List[CobolFile],
        java_files: List[JavaFile],
        dependency_map: DependencyMap,
        output_folder: str,
        session_id: str,
        start_time: datetime,
        logging_stats: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Generate comprehensive migration report.
        
        Args:
            cobol_files: Original COBOL files
            java_files: Generated Java files
            dependency_map: Dependency analysis results
            output_folder: Directory to save the report
            session_id: Migration session ID
            start_time: Migration start time
            logging_stats: Optional logging statistics from LoggingService
            
        Returns:
            Path to the generated report
        """
        output_path = Path(output_folder)
        output_path.mkdir(parents=True, exist_ok=True)
        
        report_file = output_path / f"migration_report_{session_id}.md"
        
        # Generate report content
        report_content = await self._generate_report_content(
            cobol_files=cobol_files,
            java_files=java_files,
            dependency_map=dependency_map,
            session_id=session_id,
            start_time=start_time,
            logging_stats=logging_stats
        )
        
        # Save report
        async with aiofiles.open(report_file, 'w', encoding='utf-8') as f:
            await f.write(report_content)
        
        return str(report_file)
    
    async def _generate_report_content(
        self,
        cobol_files: List[CobolFile],
        java_files: List[JavaFile],
        dependency_map: DependencyMap,
        session_id: str,
        start_time: datetime,
        logging_stats: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate the complete report content."""
        end_time = datetime.utcnow()
        duration = end_time - start_time
        
        # Calculate statistics
        stats = self._calculate_migration_statistics(cobol_files, java_files, dependency_map)
        
        content = [
            "# COBOL to Java Quarkus Migration Report",
            "",
            f"**Session ID:** `{session_id}`",
            f"**Generated:** {end_time.isoformat()}",
            f"**Migration Start:** {start_time.isoformat()}",
            f"**Migration Duration:** {self._format_duration(duration)}",
            "",
            "---",
            "",
            "## ðŸ“Š Migration Overview",
            "",
            self._generate_overview_section(stats),
            "",
            "## ðŸ¤– AI Usage & Token Statistics",
            "",
            self._generate_ai_statistics_section(logging_stats),
            "",
            "## ðŸ—‚ï¸ File Processing Summary",
            "",
            self._generate_file_summary_section(cobol_files, java_files),
            "",
            "## ðŸ”— Dependency Analysis",
            "",
            self._generate_dependency_section(dependency_map),
            "",
            "## ðŸ“ˆ Migration Metrics",
            "",
            self._generate_metrics_section(stats, duration),
            "",
            "## ðŸ“ Generated Files",
            "",
            self._generate_files_section(java_files),
            "",
            "## âš ï¸ Issues and Recommendations",
            "",
            self._generate_issues_section(java_files, dependency_map),
            "",
            "## ðŸš€ Next Steps",
            "",
            self._generate_next_steps_section(),
            "",
            "## ðŸ“‹ File Mapping Details",
            "",
            self._generate_file_mapping_section(cobol_files, java_files),
            "",
            "---",
            "",
            f"*Report generated by COBOL Migration Agents using atomic-agents framework*",
            f"*For support and documentation, visit the project repository*"
        ]
        
        return "\n".join(content)
    
    def _calculate_migration_statistics(
        self,
        cobol_files: List[CobolFile],
        java_files: List[JavaFile],
        dependency_map: DependencyMap
    ) -> Dict[str, Any]:
        """Calculate comprehensive migration statistics."""
        # Basic counts
        total_cobol_files = len(cobol_files)
        total_java_files = len(java_files)
        
        # File type breakdown
        cobol_programs = len([f for f in cobol_files if f.file_type == 'program'])
        cobol_copybooks = len([f for f in cobol_files if f.file_type == 'copybook'])
        
        # Size calculations
        total_cobol_size = sum(f.size_bytes for f in cobol_files)
        total_java_size = sum(f.size_bytes for f in java_files)
        total_cobol_lines = sum(len(f.content.split('\n')) for f in cobol_files)
        total_java_lines = sum(f.lines_of_code for f in java_files if f.lines_of_code > 0)
        
        # Quality metrics
        files_requiring_review = len([f for f in java_files if f.manual_review_required])
        average_accuracy = sum(f.estimated_accuracy for f in java_files) / len(java_files) if java_files else 0
        
        # Complexity analysis
        complexity_breakdown = {'low': 0, 'medium': 0, 'high': 0}
        for java_file in java_files:
            complexity_breakdown[java_file.migration_complexity] += 1
        
        return {
            'total_cobol_files': total_cobol_files,
            'total_java_files': total_java_files,
            'cobol_programs': cobol_programs,
            'cobol_copybooks': cobol_copybooks,
            'total_cobol_size': total_cobol_size,
            'total_java_size': total_java_size,
            'total_cobol_lines': total_cobol_lines,
            'total_java_lines': total_java_lines,
            'files_requiring_review': files_requiring_review,
            'average_accuracy': average_accuracy,
            'complexity_breakdown': complexity_breakdown,
            'conversion_rate': (total_java_files / total_cobol_files * 100) if total_cobol_files > 0 else 0
        }
    
    def _generate_overview_section(self, stats: Dict[str, Any]) -> str:
        """Generate the overview section."""
        return f"""
- **Source Files Processed:** {stats['total_cobol_files']} COBOL files
  - Programs: {stats['cobol_programs']}
  - Copybooks: {stats['cobol_copybooks']}
- **Java Files Generated:** {stats['total_java_files']}
- **Conversion Rate:** {stats['conversion_rate']:.1f}%
- **Average Accuracy:** {stats['average_accuracy']:.1f}%
- **Files Requiring Manual Review:** {stats['files_requiring_review']}

### Migration Complexity Distribution
- **Low Complexity:** {stats['complexity_breakdown']['low']} files
- **Medium Complexity:** {stats['complexity_breakdown']['medium']} files  
- **High Complexity:** {stats['complexity_breakdown']['high']} files
        """.strip()
    
    def _generate_file_summary_section(
        self,
        cobol_files: List[CobolFile],
        java_files: List[JavaFile]
    ) -> str:
        """Generate file processing summary."""
        return f"""
| Metric | COBOL Source | Java Generated |
|--------|-------------|----------------|
| **Total Files** | {len(cobol_files)} | {len(java_files)} |
| **Total Size** | {self._format_bytes(sum(f.size_bytes for f in cobol_files))} | {self._format_bytes(sum(f.size_bytes for f in java_files))} |
| **Total Lines** | {sum(len(f.content.split('\\n')) for f in cobol_files):,} | {sum(f.lines_of_code for f in java_files if f.lines_of_code > 0):,} |
| **Average File Size** | {self._format_bytes(sum(f.size_bytes for f in cobol_files) // len(cobol_files) if cobol_files else 0)} | {self._format_bytes(sum(f.size_bytes for f in java_files) // len(java_files) if java_files else 0)} |
        """.strip()
    
    def _generate_dependency_section(self, dependency_map: DependencyMap) -> str:
        """Generate dependency analysis section."""
        metrics = dependency_map.metrics
        
        content = [
            f"- **Total Dependencies:** {metrics.total_dependencies}",
            f"- **Average Dependencies per Program:** {metrics.average_dependencies_per_program:.1f}",
            f"- **Programs with No Dependencies:** {metrics.programs_with_no_dependencies}",
            f"- **Dependency Complexity Score:** {metrics.dependency_complexity_score:.2f}/1.0",
            f"- **Migration Risk Level:** {metrics.migration_risk_level.title()}",
            ""
        ]
        
        # Add Mermaid diagram if available
        if dependency_map.mermaid_diagram:
            content.extend([
                "### ðŸ—ºï¸ Dependency Visualization",
                "",
                "```mermaid",
                dependency_map.mermaid_diagram,
                "```",
                ""
            ])
        
        if metrics.circular_dependencies:
            content.extend([
                "### âš ï¸ Circular Dependencies Detected",
                ""
            ])
            for circular in metrics.circular_dependencies:
                cycle_str = " â†’ ".join(circular.cycle + [circular.cycle[0]])
                content.append(f"- **{circular.severity.title()} Severity:** {cycle_str}")
            content.append("")
        
        if metrics.most_used_copybook:
            content.extend([
                "### ðŸ“š Copybook Usage",
                "",
                f"- **Most Used Copybook:** {metrics.most_used_copybook} ({metrics.most_used_copybook_count} usages)",
                f"- **Unused Copybooks:** {len(metrics.unused_copybooks)}"
            ])
        
        # Add migration order if available
        if dependency_map.migration_order:
            content.extend([
                "",
                "### ðŸš€ Recommended Migration Order",
                ""
            ])
            for i, file_name in enumerate(dependency_map.migration_order[:10], 1):
                content.append(f"{i}. {file_name}")
            
            if len(dependency_map.migration_order) > 10:
                content.append(f"   ... and {len(dependency_map.migration_order) - 10} more files")
        
        return "\n".join(content)
    
    def _generate_metrics_section(self, stats: Dict[str, Any], duration: timedelta) -> str:
        """Generate metrics section."""
        files_per_minute = (stats['total_cobol_files'] / duration.total_seconds() * 60) if duration.total_seconds() > 0 else 0
        
        return f"""
- **Processing Speed:** {files_per_minute:.1f} files per minute
- **Code Expansion Ratio:** {(stats['total_java_lines'] / stats['total_cobol_lines']):.2f}x (Java lines / COBOL lines)
- **Size Change:** {((stats['total_java_size'] - stats['total_cobol_size']) / stats['total_cobol_size'] * 100):.1f}% {"increase" if stats['total_java_size'] > stats['total_cobol_size'] else "decrease"}
- **Review Percentage:** {(stats['files_requiring_review'] / stats['total_java_files'] * 100):.1f}% of files need manual review
        """.strip()
    
    def _generate_files_section(self, java_files: List[JavaFile]) -> str:
        """Generate generated files section."""
        # Group files by type
        file_types = {}
        for java_file in java_files:
            if java_file.is_rest_endpoint:
                file_types.setdefault('REST Endpoints', []).append(java_file)
            elif java_file.is_entity:
                file_types.setdefault('JPA Entities', []).append(java_file)
            elif java_file.is_service:
                file_types.setdefault('Service Classes', []).append(java_file)
            elif java_file.is_repository:
                file_types.setdefault('Repository Classes', []).append(java_file)
            else:
                file_types.setdefault('Other Classes', []).append(java_file)
        
        content = []
        for file_type, files in file_types.items():
            content.extend([
                f"### {file_type} ({len(files)} files)",
                ""
            ])
            
            for java_file in files[:10]:  # Limit to first 10 files
                status = "âš ï¸ Needs Review" if java_file.manual_review_required else "âœ… Ready"
                accuracy = f"{java_file.estimated_accuracy:.1%}"
                content.append(f"- **{java_file.class_name}** - {status} ({accuracy} accuracy)")
            
            if len(files) > 10:
                content.append(f"- *... and {len(files) - 10} more files*")
            
            content.append("")
        
        return "\n".join(content)
    
    def _generate_issues_section(
        self,
        java_files: List[JavaFile],
        dependency_map: DependencyMap
    ) -> str:
        """Generate issues and recommendations section."""
        content = []
        
        # Files requiring review
        review_files = [f for f in java_files if f.manual_review_required]
        if review_files:
            content.extend([
                "### Files Requiring Manual Review",
                ""
            ])
            for java_file in review_files[:5]:  # Show first 5
                content.append(f"- **{java_file.class_name}:** {', '.join(java_file.todo_items[:2])}")
            if len(review_files) > 5:
                content.append(f"- *... and {len(review_files) - 5} more files*")
            content.append("")
        
        # High complexity files
        high_complexity = [f for f in java_files if f.migration_complexity == 'high']
        if high_complexity:
            content.extend([
                "### High Complexity Conversions",
                ""
            ])
            for java_file in high_complexity[:5]:
                content.append(f"- **{java_file.class_name}:** Consider additional testing and validation")
            content.append("")
        
        # Dependency issues
        if dependency_map.metrics.circular_dependencies:
            content.extend([
                "### Dependency Issues",
                "",
                "- **Circular Dependencies:** Resolve before deployment to avoid runtime issues",
                "- **Migration Order:** Follow the recommended migration sequence in the dependency map",
                ""
            ])
        
        return "\n".join(content)
    
    def _generate_next_steps_section(self) -> str:
        """Generate next steps section."""
        return """
1. **Review Generated Code**
   - Examine files marked for manual review
   - Validate business logic accuracy
   - Test compilation and basic functionality

2. **Setup Quarkus Project**
   - Initialize Quarkus project structure
   - Configure Maven/Gradle dependencies
   - Setup application properties

3. **Testing Strategy**
   - Create unit tests for converted business logic
   - Implement integration tests
   - Perform end-to-end testing

4. **Deployment Preparation**
   - Configure production settings
   - Setup monitoring and logging
   - Plan rollback strategies

5. **Documentation**
   - Update system documentation
   - Create migration notes for future reference
   - Document any manual changes made
        """.strip()
    
    def _generate_file_mapping_section(
        self,
        cobol_files: List[CobolFile],
        java_files: List[JavaFile]
    ) -> str:
        """Generate detailed file mapping section."""
        content = [
            "| COBOL File | Java Class | Package | Type | Accuracy | Status |",
            "|------------|------------|---------|------|----------|--------|"
        ]
        
        # Create mapping
        java_by_original = {jf.original_cobol_file_name: jf for jf in java_files}
        
        for cobol_file in cobol_files:
            java_file = java_by_original.get(cobol_file.file_name)
            
            if java_file:
                file_type = "REST" if java_file.is_rest_endpoint else \
                           "Entity" if java_file.is_entity else \
                           "Service" if java_file.is_service else \
                           "Repository" if java_file.is_repository else "Class"
                
                accuracy = f"{java_file.estimated_accuracy:.0%}"
                status = "âš ï¸ Review" if java_file.manual_review_required else "âœ… Ready"
                
                content.append(
                    f"| {cobol_file.file_name} | {java_file.class_name} | {java_file.package_name} | {file_type} | {accuracy} | {status} |"
                )
            else:
                content.append(
                    f"| {cobol_file.file_name} | âŒ Not Generated | - | - | - | Failed |"
                )
        
        return "\n".join(content)
    
    def _format_duration(self, duration: timedelta) -> str:
        """Format duration in a human-readable way."""
        total_seconds = int(duration.total_seconds())
        hours = total_seconds // 3600
        minutes = (total_seconds % 3600) // 60
        seconds = total_seconds % 60
        
        if hours > 0:
            return f"{hours}h {minutes}m {seconds}s"
        elif minutes > 0:
            return f"{minutes}m {seconds}s"
        else:
            return f"{seconds}s"
    
    def _format_bytes(self, bytes_count: int) -> str:
        """Format bytes in a human-readable way."""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if bytes_count < 1024.0:
                return f"{bytes_count:.1f} {unit}"
            bytes_count /= 1024.0
        return f"{bytes_count:.1f} TB"
    
    def _generate_ai_statistics_section(self, logging_stats: Optional[Dict[str, Any]]) -> str:
        """Generate AI usage and token statistics section."""
        if not logging_stats:
            return "No AI usage statistics available."
        
        content = []
        
        # Overall statistics
        content.extend([
            "### ðŸ“Š Overall AI Usage",
            "",
            f"- **Total API Calls:** {logging_stats.get('api_calls', 0)}",
            f"- **Hook-Captured Calls:** {logging_stats.get('hook_captured_calls', 0)}",
            f"- **Total Session Duration:** {logging_stats.get('duration_seconds', 0):.1f} seconds",
            f"- **Average Call Duration:** {logging_stats.get('average_call_duration', 0):.2f} seconds",
            f"- **Errors:** {logging_stats.get('errors', 0)}",
            ""
        ])
        
        # Token statistics
        content.extend([
            "### ðŸ”¢ Token Usage",
            "",
            f"- **Total Prompt Tokens:** {logging_stats.get('total_prompt_tokens', 0):,}",
            f"- **Total Completion Tokens:** {logging_stats.get('total_completion_tokens', 0):,}",
            f"- **Total Tokens:** {logging_stats.get('total_tokens', 0):,}",
            f"- **Estimated Total Cost:** ${logging_stats.get('total_cost', 0):.4f}",
            ""
        ])
        
        # Agent breakdown
        agent_breakdown = logging_stats.get('agent_breakdown', {})
        if agent_breakdown:
            content.extend([
                "### ðŸ¤– Agent Performance Breakdown",
                "",
                "| Agent | Calls | Prompt Tokens | Completion Tokens | Total Tokens | Cost ($) | Avg Duration (s) |",
                "|-------|-------|---------------|-------------------|--------------|----------|------------------|"
            ])
            
            for agent_name, agent_stats in agent_breakdown.items():
                content.append(
                    f"| {agent_name} | {agent_stats.get('calls', 0)} | "
                    f"{agent_stats.get('prompt_tokens', 0):,} | "
                    f"{agent_stats.get('completion_tokens', 0):,} | "
                    f"{agent_stats.get('total_tokens', 0):,} | "
                    f"{agent_stats.get('cost', 0):.4f} | "
                    f"{agent_stats.get('avg_duration', 0):.2f} |"
                )
            
            content.append("")
        
        # Efficiency metrics
        total_tokens = logging_stats.get('total_tokens', 0)
        total_calls = logging_stats.get('api_calls', 0)
        total_duration = logging_stats.get('duration_seconds', 0)
        
        if total_calls > 0 and total_duration > 0:
            content.extend([
                "### âš¡ Efficiency Metrics",
                "",
                f"- **Tokens per API Call:** {total_tokens / total_calls:.1f}",
                f"- **Tokens per Second:** {total_tokens / total_duration:.1f}",
                f"- **API Calls per Minute:** {(total_calls * 60) / total_duration:.1f}",
                ""
            ])
        
        # Hook status
        hooks_enabled = logging_stats.get('hooks_enabled', False)
        content.extend([
            "### ðŸ”— Hook Status",
            "",
            f"- **Instructor Hooks Enabled:** {'âœ… Yes' if hooks_enabled else 'âŒ No'}",
            f"- **Automatic Token Tracking:** {'âœ… Active' if hooks_enabled else 'âŒ Inactive'}",
            ""
        ])
        
        if not hooks_enabled:
            content.extend([
                "> âš ï¸ **Note:** Instructor hooks are not enabled. Token tracking may be incomplete.",
                "> Enable hooks for automatic and accurate token consumption tracking.",
                ""
            ])
        
        return "\n".join(content)